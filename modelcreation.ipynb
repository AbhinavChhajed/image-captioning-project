{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaf51abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "606d040b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.50s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = 'data'\n",
    "img_dir = os.path.join(data_dir, 'train2017','train2017')\n",
    "ann_file = os.path.join(data_dir,'annotations_trainval2017', 'annotations', 'captions_train2017.json')\n",
    "\n",
    "# === Load COCO captions ===\n",
    "coco = COCO(ann_file)\n",
    "img_ids = coco.getImgIds()\n",
    "\n",
    "# === Choose 100 random image IDs ===\n",
    "selected_img_ids = random.sample(img_ids, 100)\n",
    "\n",
    "samples = []\n",
    "\n",
    "for img_id in selected_img_ids:\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    file_name = img_info['file_name']\n",
    "    file_path = os.path.join(img_dir, file_name)\n",
    "\n",
    "    # Load the first caption (or more if needed)\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    caption = anns[0]['caption'] if anns else \"\"\n",
    "\n",
    "    # Load image using PIL (or use tf.io.read_file if you prefer tensors)\n",
    "    image = Image.open(file_path).convert('RGB')\n",
    "\n",
    "    samples.append({\n",
    "        'image': image,\n",
    "        'caption': caption,\n",
    "        'file_path': file_path,\n",
    "    })\n",
    "\n",
    "# âœ… Now 'samples' is a list of 100 (image, caption) dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c4ba817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_image_tensor(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    "\n",
    "# Convert to tf.data.Dataset if needed\n",
    "file_paths = [sample['file_path'] for sample in samples]\n",
    "captions = [sample['caption'] for sample in samples]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((file_paths, captions))\n",
    "\n",
    "def process(path, caption):\n",
    "    image = load_image_tensor(path)\n",
    "    return {'image': image, 'captions': {'text': caption}}\n",
    "\n",
    "dataset = dataset.map(process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fbeada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image model (encoder)\n",
    "encoder = EfficientNetB3(include_top=False,pooling='avg')\n",
    "encoder.trainable = False\n",
    "max_token = 10000\n",
    "max_length = 30\n",
    "vectorizer = TextVectorization(max_tokens=max_token, output_sequence_length=max_length+1)\n",
    "captions = dataset.map(lambda x: x['captions']['text'])\n",
    "vectorizer.adapt(captions)\n",
    "vocab_size = vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36e23487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature pre processing\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d58f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(2,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516fdad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
