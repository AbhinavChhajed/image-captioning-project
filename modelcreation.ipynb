{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf51abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import LayerNormalization,Dense,Embedding,MultiHeadAttention,Dropout,Layer\n",
    "from tensorflow.keras import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = 'data'\n",
    "img_dir = os.path.join(data_dir, 'train2017','train2017')\n",
    "ann_file = os.path.join(data_dir,'annotations_trainval2017', 'annotations', 'captions_train2017.json')\n",
    "\n",
    "# === Load COCO captions ===\n",
    "coco = COCO(ann_file)\n",
    "img_ids = coco.getImgIds()\n",
    "\n",
    "# === Choose 100 random image IDs ===\n",
    "selected_img_ids = random.sample(img_ids, 100)\n",
    "\n",
    "samples = []\n",
    "\n",
    "for img_id in selected_img_ids:\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    file_name = img_info['file_name']\n",
    "    file_path = os.path.join(img_dir, file_name)\n",
    "\n",
    "    # Load the first caption (or more if needed)\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    caption = anns[0]['caption'] if anns else \"\"\n",
    "\n",
    "    # Load image using PIL (or use tf.io.read_file if you prefer tensors)\n",
    "    image = Image.open(file_path).convert('RGB')\n",
    "\n",
    "    samples.append({\n",
    "        'image': image,\n",
    "        'caption': caption,\n",
    "        'file_path': file_path,\n",
    "    })\n",
    "\n",
    "# âœ… Now 'samples' is a list of 100 (image, caption) dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ba817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_image_tensor(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    "\n",
    "# Convert to tf.data.Dataset if needed\n",
    "file_paths = [sample['file_path'] for sample in samples]\n",
    "captions = [sample['caption'] for sample in samples]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((file_paths, captions))\n",
    "\n",
    "def process(path, caption):\n",
    "    image = load_image_tensor(path)\n",
    "    return {'image': image, 'captions': {'text': caption}}\n",
    "\n",
    "dataset = dataset.map(process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbeada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image model (encoder)\n",
    "encoder = EfficientNetB3(include_top=False,pooling='avg')\n",
    "encoder.trainable = False\n",
    "max_token = 10000\n",
    "max_length = 30\n",
    "vectorizer = TextVectorization(max_tokens=max_token, output_sequence_length=max_length+2)\n",
    "captions = dataset.map(lambda x: x['captions']['text'])\n",
    "vectorizer.adapt(captions)\n",
    "vocab_size = vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e23487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature pre processing\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sam = random.choice(samples)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(sam['image'])\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Image\")\n",
    "plt.show()\n",
    "print(sam['caption'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder Layer\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self,model_dim,self_attention_num,feed_forward_dim,Rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(num_heads=self_attention_num,key_dim=model_dim)\n",
    "        self.cross_attention = MultiHeadAttention(num_heads=self_attention_num,key_dim=model_dim)\n",
    "        self.feed_forward_nn = tf.keras.Sequential([Dense(feed_forward_dim, activation=\"relu\"), Dense(model_dim)])\n",
    "        self.norm1 = LayerNormalization()\n",
    "        self.norm2 = LayerNormalization()\n",
    "        self.norm3 = LayerNormalization()\n",
    "        self.drop1 = Dropout(Rate)\n",
    "        self.drop2 = Dropout(Rate)\n",
    "        self.drop3 = Dropout(Rate)\n",
    "    def call(self, x, img_feature, training):\n",
    "        att1 = self.self_attention(x,x,use_causal_mask=True)\n",
    "        att1 = self.drop1(att1,training=training)\n",
    "        out1 = self.norm1(x+att1)\n",
    "\n",
    "        att2 = self.cross_attention(out1,img_feature)\n",
    "        att2 = self.drop2(att2,training=training)\n",
    "        out2 = self.norm2(out1+att2)\n",
    "\n",
    "        ffn = self.feed_forward_nn(out2)\n",
    "        ffn = self.drop3(ffn,training=training)\n",
    "        out3 = self.norm3(out2+ffn)\n",
    "\n",
    "        return out3\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaeb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "class TransformerDcoder(Layer):\n",
    "    def __init__(self,model_dim,num_layers,vocabulary_size,positional_encoding_size,feed_forward_dim,self_attention_num,Rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(vocabulary_size,model_dim)\n",
    "        self.pos_embedding = Embedding(positional_encoding_size,model_dim)\n",
    "        self.decoder_layers = [DecoderLayer(model_dim=model_dim,self_attention_num=self_attention_num,Rate=Rate,feed_forward_dim=feed_forward_dim) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(Rate)\n",
    "\n",
    "    def call(self,target,img_feature,training):\n",
    "        seq_len = tf.shape(target)[1]\n",
    "        x = self.embedding(target) * tf.math.sqrt(tf.cast(self.model_dim, tf.float32))\n",
    "        positions = tf.range(start=0, limit=seq_len, delta=1)[tf.newaxis, :]\n",
    "        x += self.pos_embedding(positions)\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.decoder_layers[i](x,img_feature,training)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f84b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "\n",
    "def ImgEncoder():\n",
    "    \n",
    "    base_model = EfficientNetB3(include_top=False,weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    output = base_model.output\n",
    "    output = tf.keras.layers.Reshape((-1, output.shape[-1]))(output)\n",
    "    \n",
    "    encoder = Model(base_model.input,output)\n",
    "    return encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main model\n",
    "class MyModel(Model):\n",
    "    def __init__(self,ImgEncoder,decoder,vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = ImgEncoder\n",
    "        self.decoder = decoder\n",
    "        self.dense = Dense(vocab_size)\n",
    "    \n",
    "    def call(self,input,target,training):\n",
    "        img_feature = self.encoder(input)\n",
    "        decoder_output = self.decoder(target, img_feature, training)\n",
    "        return self.dense(decoder_output)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see for padding later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 512\n",
    "num_layers = 4\n",
    "vocab_size = 10000\n",
    "pos_encoding_size = 1000\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "decoder = TransformerDcoder(model_dim=model_dim,num_layers=num_layers,vocabulary_size=vocab_size\n",
    "                            ,positional_encoding_size=pos_encoding_size,feed_forward_dim=dff\n",
    "                            ,self_attention_num=num_heads,Rate=dropout_rate)\n",
    "encoder = ImgEncoder()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d86373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(path, caption):\n",
    "    image = load_image_tensor(path)\n",
    "    image = tf.image.resize(image, (300, 300))  # EfficientNetB3 uses 300x300\n",
    "    image = preprocess_input(image)\n",
    "    return {'image': image, 'captions': {'text': caption}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79aeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(ImgEncoder=encoder,vocab_size=vocab_size,decoder=decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5333e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
